## Introduction

Out of all the 3 models I modified, Logistic Regression is the most straightforward one. Logistic regression models the log-odds of an event as a linear combination of one or more independent variable (Wikipedia 2024). Therefore, the parameters are w, weights assigned to each feature, and b, bias. The initialization of parameters includes equal weight w and b as 0s, cross validation with 5 splits, and Adam optimizer from Torch library with 0.001 learning rate. 

## F-1 Method

The model is trained with training loop to update weights w and b using Gradient-Based Optimizer. In this model, to minimize the loss is to maximize the negative value of the output from F-1 method. During each single fold, the logits set, generated from updated parameters, is applied to sigmoid function to calculate probabilities. Then the probabilities are compared to a threshold to classify dataset. Results were generated after 5-fold of cross validation to compare the true labels with predicted labels: 

When tested on a balanced dataset, the average F-1 is 0.9243, with the score of each fold being 0.9016, 0.9207, 0.9336, 0.9378, and 0.9277.

When tested on an imbalanced dataset, the average F-1 is 0.8600, with the score of each fold being 0.8276, 0.8489, 0.8787, 0.8714, and 0.8735.

[code](Logistic_regression_f1.py) 


## Cross-entropy

Inside of 5-fold cross-validation training loop, parameters w and b are trained using gradient-based optimization and using cross-entropy as loss function. During each evaluation, the model compares probabilities, generated by the sigmoid function, to a certain threshold. Then calculate the F-1 score from each fold. Results were generated after 5-folds of cross validation:
 
When tested on a balanced dataset, the average F-1 score is 0.8235, with the score of each fold being 0.9095, 0.6761, 0.9173, 0.9145, and 0.7003.

When tested on an imbalanced dataset, the average F-1 score is 0.7315, with the score of each fold being 0.7318, 0.7425, 0.7121, 0.6633, and 0.8076.

[code](Logistic_regression_cross_entropy.py) 

## Observation and Interpretation

Supported by the data, F-1 method performed 12.24% better than cross-entropy method on balanced data and 31.34% better on imbalanced data. F-1 method generally has a better performance and works very well on imbalanced data set. This is because when the model uses cross-entropy as a loss function, it treats all errors equally, without considering the class distribution. In other words, the model prioritizes the majority class, minimizing overall loss, without trying to minimize FP, or FN. This is easy to understand by observing the equation. The loss function evaluates only on the predicted probability of the true class, ignoring the probabilities for the incorrect classes. F-1 method on the other hand, focuses on minority class, in a way that it also penalizes the model when FP and FN present.  
 
Another point that stood out is that cross-entropy loss penalizes greatly on misclassified data point with high confidence, while F-1 method does not. F-1 method focuses on probabilistic evaluation. This way, model using F-1 method is still being encouraged (compared to the traditional method) to predict minority class outcomes. This aim is reinforced by trying to minimize FP and FN.  
